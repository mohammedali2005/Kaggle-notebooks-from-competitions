{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":90087,"databundleVersionId":11467685,"sourceType":"competition"}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dataaddict71/fork-of-optuna-lasso-rdkit-ensemble?scriptVersionId=236228866\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-25T15:54:42.150521Z","iopub.execute_input":"2025-04-25T15:54:42.150684Z","iopub.status.idle":"2025-04-25T15:54:43.84636Z","shell.execute_reply.started":"2025-04-25T15:54:42.150668Z","shell.execute_reply":"2025-04-25T15:54:43.845727Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T16:00:38.17297Z","iopub.execute_input":"2025-04-25T16:00:38.173535Z","iopub.status.idle":"2025-04-25T16:00:43.864699Z","shell.execute_reply.started":"2025-04-25T16:00:38.173506Z","shell.execute_reply":"2025-04-25T16:00:43.863972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport warnings\nimport time\n\n# RDKit for FE\nfrom rdkit import Chem\nfrom rdkit.Chem import Descriptors, rdMolDescriptors, AllChem\nfrom rdkit.ML.Descriptors import MoleculeDescriptors\n\n# Preprocessing & Feature Selection\nfrom sklearn.preprocessing import StandardScaler, QuantileTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.feature_selection import VarianceThreshold, SelectFromModel\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import LeaveOneOut, KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_log_error, make_scorer\n\n# Models\nfrom sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:33:23.820518Z","iopub.execute_input":"2025-04-25T23:33:23.820794Z","iopub.status.idle":"2025-04-25T23:33:23.825905Z","shell.execute_reply.started":"2025-04-25T23:33:23.820775Z","shell.execute_reply":"2025-04-25T23:33:23.825342Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Hyperparameter Optimization\nimport optuna\noptuna.logging.set_verbosity(optuna.logging.WARNING) # Reduce Optuna verbosity\n\n# Settings\nDATA_PATH = '/kaggle/input/molecular-machine-learning/' \nSEED = 42 # For reproducibility\nN_OPTUNA_TRIALS = 100 # Number of trials for HPO \n#CV_STRATEGY = LeaveOneOut() # Robust for N=42\n\nN_SPLITS = 7 # Using 7 folds for N=42 (each validation set has 6 samples)\nCV_STRATEGY = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n# Ignore common warnings\nwarnings.filterwarnings('ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore', category=UserWarning)\nwarnings.filterwarnings(\"ignore\", message=\".*Consider increasing the value of the `num_leaves` parameter.*\")\nwarnings.filterwarnings(\"ignore\", message=\".*Using `tqdm.autonotebook.tqdm` in notebook mode.*\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:33:25.553135Z","iopub.execute_input":"2025-04-25T23:33:25.553852Z","iopub.status.idle":"2025-04-25T23:33:25.558688Z","shell.execute_reply.started":"2025-04-25T23:33:25.553826Z","shell.execute_reply":"2025-04-25T23:33:25.558107Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --1. Load Data ---\nprint(\"1. Loading data...\")\ntrain_df = pd.read_csv(DATA_PATH + 'train.csv')\ntest_df = pd.read_csv(DATA_PATH + 'test.csv')\nsample_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\n\nprint(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n\n# Separate target variable\ny = train_df['T80']\n# Apply log1p transformation - crucial for MSLE metric\ny_log = np.log1p(y)\n\n# Store IDs for final submission\ntrain_ids = train_df['Batch_ID']\ntest_ids = test_df['Batch_ID']","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:33:26.857571Z","iopub.execute_input":"2025-04-25T23:33:26.858082Z","iopub.status.idle":"2025-04-25T23:33:26.886657Z","shell.execute_reply.started":"2025-04-25T23:33:26.858059Z","shell.execute_reply":"2025-04-25T23:33:26.886102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 2. Feature Engineering with RDKit ---\nprint(\"\\n2. Generating RDKit features...\")\n\ndef generate_rdkit_features(smiles_list):\n    \"\"\"Generates a wide range of RDKit descriptors for a list of SMILES.\"\"\"\n    mols = [Chem.MolFromSmiles(s) for s in smiles_list]\n    failed_mols = [i for i, mol in enumerate(mols) if mol is None]\n    if failed_mols:\n        print(f\"Warning: Could not parse SMILES for indices: {failed_mols}\")\n        # Handle failed molecules - here we fill with NaNs later\n        # Alternatively, could try sanitizing SMILES or return default values\n\n    # Use a predefined list of RDKit descriptors\n    desc_list = [desc_name[0] for desc_name in Descriptors._descList]\n    calculator = MoleculeDescriptors.MolecularDescriptorCalculator(desc_list)\n\n    features = []\n    for i, mol in enumerate(mols):\n        if mol:\n            try:\n                # Generate 3D coordinates for descriptors that need them\n                mol_h = Chem.AddHs(mol)\n                AllChem.EmbedMolecule(mol_h, randomSeed=SEED)\n                AllChem.MMFFOptimizeMolecule(mol_h) # Basic optimization\n                mol = Chem.RemoveHs(mol_h)\n\n                mol_features = calculator.CalcDescriptors(mol)\n                # Add some additional potentially useful descriptors\n                mol_features = list(mol_features)\n                mol_features.append(rdMolDescriptors.CalcNumAtomStereoCenters(mol))\n                mol_features.append(rdMolDescriptors.CalcNumUnspecifiedAtomStereoCenters(mol))\n                mol_features.append(rdMolDescriptors.CalcNumBridgeheadAtoms(mol))\n                mol_features.append(rdMolDescriptors.CalcNumSpiroAtoms(mol))\n                mol_features.append(AllChem.GetBestRMS(mol_h, mol_h)) # Can indicate flexibility/strain after opt\n                features.append(mol_features)\n            except Exception as e:\n                # print(f\"Warning: RDKit descriptor calculation failed for index {i}: {e}\")\n                # Pad with NaNs if calculation fails for a molecule\n                features.append([np.nan] * (len(calculator.GetDescriptorNames()) + 5))\n        else:\n             # Pad with NaNs if Mol object couldn't be created\n            features.append([np.nan] * (len(calculator.GetDescriptorNames()) + 5))\n\n    # Add names for the extra descriptors\n    extra_desc_names = ['NumAtomStereoCenters', 'NumUnspecifiedAtomStereoCenters',\n                        'NumBridgeheadAtoms', 'NumSpiroAtoms', 'BestRMS_MMFF']\n    feature_names = list(calculator.GetDescriptorNames()) + extra_desc_names\n\n    rdkit_df = pd.DataFrame(features, columns=feature_names)\n    return rdkit_df\n\n# Generate features for train and test\ntrain_rdkit_feats = generate_rdkit_features(train_df['Smiles'])\ntest_rdkit_feats = generate_rdkit_features(test_df['Smiles'])\n\n# Combine original features with RDKit features\n# Drop original Smiles and T80 (keep Batch_ID for joining if needed, but drop later for modeling)\nX = train_df.drop(columns=['Batch_ID', 'T80', 'Smiles']).copy()\nX_test = test_df.drop(columns=['Batch_ID', 'Smiles']).copy()\n\n# Ensure indices align before joining\nX.reset_index(drop=True, inplace=True)\nX_test.reset_index(drop=True, inplace=True)\ntrain_rdkit_feats.reset_index(drop=True, inplace=True)\ntest_rdkit_feats.reset_index(drop=True, inplace=True)\n\n# Add prefix to avoid potential name collisions\ntrain_rdkit_feats = train_rdkit_feats.add_prefix('RDKIT_')\ntest_rdkit_feats = test_rdkit_feats.add_prefix('RDKIT_')\n\nX = pd.concat([X, train_rdkit_feats], axis=1)\nX_test = pd.concat([X_test, test_rdkit_feats], axis=1)\n\nprint(f\"Shape after RDKit FE - Train: {X.shape}, Test: {X_test.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:33:29.107104Z","iopub.execute_input":"2025-04-25T23:33:29.107789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#  3. Preprocessing ---\nprint(\"\\n3. Preprocessing features...\")\n\n\nX.columns = X.columns.astype(str)\nX_test.columns = X_test.columns.astype(str)\n\n\ncommon_cols = list(set(X.columns) & set(X_test.columns))\nX = X[common_cols]\nX_test = X_test[common_cols]\nprint(f\"Shape after aligning columns - Train: {X.shape}, Test: {X_test.shape}\")\n\n\nprint(f\"   - Columns before deduplication: {X.shape[1]}\")\n# Check for duplicates\nduplicated_cols = X.columns[X.columns.duplicated()].unique()\nif len(duplicated_cols) > 0:\n    print(f\"   - Found duplicate columns: {duplicated_cols.tolist()}\")\n    \n    X = X.loc[:, ~X.columns.duplicated(keep='first')]\n    X_test = X_test.loc[:, ~X_test.columns.duplicated(keep='first')]\n    print(f\"   - Columns after deduplication: {X.shape[1]}\")\n\n    \n    common_cols_dedup = list(set(X.columns) & set(X_test.columns))\n    X = X[common_cols_dedup]\n    X_test = X_test[common_cols_dedup]\n    print(f\"   - Shape after re-aligning post-deduplication - Train: {X.shape}, Test: {X_test.shape}\")\nelse:\n    print(\"   - No duplicate columns found.\")\n\n\n\nfeature_names = X.columns.tolist()\n\n\npreprocessor = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')), # Median is robust to outliers\n    ('variance_thresh', VarianceThreshold(threshold=0.01)), # Remove low-variance feats\n    ('scaler', QuantileTransformer(output_distribution='normal', n_quantiles=min(30, X.shape[0]-1), random_state=SEED)) # Robust scaling\n])\n\n\n\n\n# Fit preprocessor ON TRAINING DATA\nprint(\"   - Fitting preprocessor...\")\nX_processed = preprocessor.fit_transform(X)\nX_test_processed = preprocessor.transform(X_test)\n\n# Get feature names after VarianceThreshold\nfeature_names_after_vt = X.columns[preprocessor.named_steps['variance_thresh'].get_support()]\nprint(f\"   - Features remaining after variance threshold: {len(feature_names_after_vt)}\")\n\n# Convert back to DataFrame for easier handling \nX = pd.DataFrame(X_processed, columns=feature_names_after_vt, index=X.index)\nX_test = pd.DataFrame(X_test_processed, columns=feature_names_after_vt, index=X_test.index)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 4. Feature Selection (using LassoCV as an example selector) ---\n# This is a crucial step. We'll use LassoCV for its embedded selection.\n# We fit it once here to get an idea of selected features, but the proper\n# selection should happen within the CV loop during HPO or final model training.\nprint(\"\\n4. Preliminary Feature Selection Check (LassoCV)...\")\n\n# Use LassoCV to find a good alpha and identify potentially important features\n# Fit on the log-transformed target\nlasso_selector = LassoCV(cv=CV_STRATEGY, random_state=SEED, n_jobs=-1, max_iter=5000)\nlasso_selector.fit(X, y_log)\n\nselector_model = SelectFromModel(lasso_selector, prefit=True, threshold=1e-5) # Keep features with non-zero coefficients\n\nX_selected = selector_model.transform(X)\nX_test_selected = selector_model.transform(X_test)\n\nselected_features = X.columns[selector_model.get_support()]\nprint(f\"   - Features selected by LassoCV: {len(selected_features)}\")\nprint(f\"   - Selected Features: {selected_features.tolist()}\")\n\n# Update X and X_test to use only selected features for subsequent steps\n\nX = X[selected_features]\nX_test = X_test[selected_features]\nfeature_names = selected_features.tolist() # Update feature names list","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### ---5. Model Definition and Hyperparameter Optimization (Optuna) \nprint(\"\\n5. Defining Models and Setting up Hyperparameter Optimization...\")\n\n# MSLE Scorer for Optuna/CV (uses log1p implicitly if y is positive)\ndef msle_scorer(y_true, y_pred):\n     # Ensure non-negative predictions before log1p in case model predicts < 0\n     y_pred_non_negative = np.maximum(1e-9, y_pred) # Add small epsilon\n     # We are predicting log1p(T80), so y_true is already log1p(T80)\n     # We need to inverse transform predictions before calculating MSLE on original scale\n     return mean_squared_log_error(np.expm1(y_true), np.expm1(y_pred_non_negative))\n\n# Use neg_mean_squared_error on the log1p scale as the CV scoring objective\n# This is mathematically sound and simpler than custom MSLE scorer during CV\ncv_scoring = 'neg_mean_squared_error' \n\n# --- Model 1: SVR ---\ndef svr_objective(trial):\n    # Define hyperparameters to search\n    svr_c = trial.suggest_float('C', 1e-2, 1e3, log=True)\n    svr_gamma = trial.suggest_float('gamma', 1e-4, 1e1, log=True)\n    svr_epsilon = trial.suggest_float('epsilon', 1e-3, 1e0, log=True)\n\n    model = SVR(kernel='rbf', C=svr_c, gamma=svr_gamma, epsilon=svr_epsilon)\n\n    # Cross-validate the model\n    scores = cross_val_score(model, X, y_log, cv=CV_STRATEGY, scoring=cv_scoring, n_jobs=-1)\n    return np.mean(scores) # Optuna minininimizes objective, but we maximize neg_mse (minimize MSE)\n\n\n# --- Model 2: LightGBM ---\ndef lgbm_objective(trial):\n    # Define hyperparameters to search \n    params = {\n        'objective': 'regression_l1', # L1 loss often more robust\n        'metric': 'rmse',\n        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.2, log=True),\n        'num_leaves': trial.suggest_int('num_leaves', 5, 30), # Keep low for small N\n        'max_depth': trial.suggest_int('max_depth', 3, 8),   # Keep low\n        'min_child_samples': trial.suggest_int('min_child_samples', 5, 20), # Increase for small N\n        'subsample': trial.suggest_float('subsample', 0.5, 1.0), # Feature fraction\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0), # Feature fraction\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-2, 10.0, log=True), # L1 reg\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-2, 10.0, log=True), # L2 reg\n        'random_state': SEED,\n        'n_jobs': -1,\n        'verbose': -1 # Suppress LightGBM verbosity during HPO\n    }\n    model = LGBMRegressor(**params)\n    scores = cross_val_score(model, X, y_log, cv=CV_STRATEGY, scoring=cv_scoring, n_jobs=-1)\n    return np.mean(scores)\n\n\n# --- Run HPO ---\nprint(\"   - Running HPO for SVR...\")\nstart_time = time.time()\nsvr_study = optuna.create_study(direction='maximize') # Maximize neg_mse\nsvr_study.optimize(svr_objective, n_trials=N_OPTUNA_TRIALS, n_jobs=1) # n_jobs=1 for study for safety with CV inside\nprint(f\"   - SVR HPO finished in {time.time() - start_time:.2f}s\")\nprint(f\"   - Best SVR CV Score (neg_mse): {svr_study.best_value:.5f}\")\nprint(f\"   - Best SVR Params: {svr_study.best_params}\")\n\nprint(\"   - Running HPO for LightGBM...\")\nstart_time = time.time()\nlgbm_study = optuna.create_study(direction='maximize')\nlgbm_study.optimize(lgbm_objective, n_trials=N_OPTUNA_TRIALS, n_jobs=1)\nprint(f\"   - LightGBM HPO finished in {time.time() - start_time:.2f}s\")\nprint(f\"   - Best LGBM CV Score (neg_mse): {lgbm_study.best_value:.5f}\")\nprint(f\"   - Best LGBM Params: {lgbm_study.best_params}\")\n\n\n# --- Model 3: RidgeCV (no explicit HPO needed, finds alpha internally) ---\nprint(\"   - Evaluating RidgeCV...\")\nridge_model = RidgeCV(alphas=np.logspace(-4, 4, 100), cv=CV_STRATEGY, scoring=cv_scoring)\nridge_model.fit(X, y_log) # Fit to get internal CV score estimate (approximate)\n# Re-calculate CV score properly for comparison\nridge_cv_scores = cross_val_score(ridge_model, X, y_log, cv=CV_STRATEGY, scoring=cv_scoring, n_jobs=-1)\nridge_best_cv_score = np.mean(ridge_cv_scores)\nprint(f\"   - RidgeCV Best CV Score (neg_mse): {ridge_best_cv_score:.5f}\")\nprint(f\"   - RidgeCV Best Alpha: {ridge_model.alpha_}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 6. Final Model Training & Prediction ---\nprint(\"\\n6. Training Final Models and Predicting...\")\n\n# Instantiate models with best parameters found\nbest_svr = SVR(**svr_study.best_params)\nbest_lgbm = LGBMRegressor(objective='regression_l1', metric='rmse', random_state=SEED, n_jobs=-1, verbose=-1,\n                          **lgbm_study.best_params)\n# RidgeCV is already 'trained' in a sense, but we refit on all data\nbest_ridge = RidgeCV(alphas=[ridge_model.alpha_], cv=None, scoring=cv_scoring) # Use only best alpha\n\n\n# List of final models for ensembling\nfinal_models = {\n    'SVR': best_svr,\n    'LGBM': best_lgbm,\n    'Ridge': best_ridge\n}\n\noof_predictions = pd.DataFrame()\ntest_predictions = pd.DataFrame()\n# Train each model on the full (preprocessed, selected) training data\nfor name, model in final_models.items():\n    print(f\"   - Training final {name} model...\")\n    start_time = time.time()\n    model.fit(X, y_log)\n    print(f\"     Done in {time.time() - start_time:.2f}s\")\n    # Predict on test set (log1p scale)\n    test_preds = model.predict(X_test)\n    test_predictions[name] = test_preds\n\n   ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- 7. Ensembling ---\nprint(\"\\n7. Ensembling Predictions...\")\n\n# Simple Averaging Ensemble\nensemble_preds_log = test_predictions.mean(axis=1)\n\n# Inverse transform the predictions back to the original T80 scale\nensemble_preds = np.expm1(ensemble_preds_log)\n\n# Post-processing: Ensure predictions are non-negative\nensemble_preds[ensemble_preds < 0] = 0\n\nprint(\"   - Ensemble predictions generated.\")\nprint(f\"   - Sample Predictions:\\n{ensemble_preds.head()}\")\n\n# --- 8. Create Submission File ---\nprint(\"\\n8. Creating submission file...\")\n\nsubmission_df = pd.DataFrame({'Batch_ID': test_ids, 'T80': ensemble_preds})\nsubmission_df.to_csv('submission.csv', index=False)\n\nprint(\"\\nPipeline Complete. Submission file 'submission.csv' created.\")\n\n# --- Optional: Analyze Feature Importances (from LGBM) ---\ntry:\n    if 'LGBM' in final_models:\n        lgbm_final = final_models['LGBM']\n        importances = pd.DataFrame({\n            'feature': feature_names,\n            'importance': lgbm_final.feature_importances_\n        }).sort_values('importance', ascending=False)\n\n        print(\"\\nTop 15 Features from final LightGBM model:\")\n        print(importances.head(15))\nexcept Exception as e:\n    print(f\"\\nCould not generate feature importances: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-25T23:32:40.394053Z","iopub.execute_input":"2025-04-25T23:32:40.394665Z","iopub.status.idle":"2025-04-25T23:32:40.40738Z","shell.execute_reply.started":"2025-04-25T23:32:40.394628Z","shell.execute_reply":"2025-04-25T23:32:40.406517Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}