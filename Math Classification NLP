{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":97669,"databundleVersionId":11615683,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/dataaddict71/math-classification-nlp?scriptVersionId=240683462\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:23:11.786611Z","iopub.execute_input":"2025-05-19T08:23:11.787373Z","iopub.status.idle":"2025-05-19T08:23:12.09016Z","shell.execute_reply.started":"2025-05-19T08:23:11.787341Z","shell.execute_reply":"2025-05-19T08:23:12.089436Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\n\"\"\"\nFinal Ensemble Pipeline for Math Problem Classification\n\nThis notebook implements a multi-stage ensemble approach to classify math problems\ninto one of eight predefined categories. The pipeline integrates:\n1. Advanced math-aware text preprocessing.\n2. (Optional) Contrastive pre-training of a Transformer model.\n3. Fine-tuning a Transformer model (e.g., DeBERTa-v3-base) using Layer-wise \n   Learning Rate Decay (LLRD) and K-Fold Cross-Validation.\n4. (Optional) Training a LightGBM model on engineered symbolic and TF-IDF features.\n5. Ensembling predictions from the Transformer and (optionally) the symbolic model.\n\nThe evaluation metric is F1-micro.\n\"\"\"\n\n# Core Libraries\nimport os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport re\nimport random\nimport time\nimport shutil\nimport logging\nfrom typing import Optional, Tuple, Dict, Any, List\n\n# Scikit-learn for modeling and metrics\nfrom sklearn.model_selection import StratifiedKFold, train_test_split # Added train_test_split\nfrom sklearn.metrics import f1_score\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.special import softmax # For converting logits to probabilities\n\n# PyTorch\nimport torch\nfrom torch import nn\nfrom torch.optim import AdamW\nfrom torch.utils.data import DataLoader as TorchDataLoader # Renamed to avoid conflict with HF\n\n# Hugging Face Transformers & Datasets\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForSequenceClassification,\n    TrainingArguments,\n    Trainer,\n    DataCollatorWithPadding,\n    EarlyStoppingCallback,\n    TrainerCallback,\n    logging as hf_logging,\n    get_scheduler\n)\nfrom datasets import Dataset as HFDataset # Renamed to avoid conflict with torch.utils.data.Dataset\nfrom datasets import DatasetDict\n\n# Sentence Transformers for optional contrastive learning stage\n# This is an optional dependency.\ntry:\n    from sentence_transformers import SentenceTransformer, InputExample, losses, models\n    SENTENCE_TRANSFORMERS_AVAILABLE = True\nexcept ImportError:\n    SENTENCE_TRANSFORMERS_AVAILABLE = False\n    print(\"Warning: sentence-transformers library not found. Contrastive pre-training stage will be disabled.\")\n\n# LightGBM for the symbolic model\nimport lightgbm as lgb\n\n# ==============================================================================\n# Configuration\n# ==============================================================================\n# All major hyperparameters and settings are managed in this CONFIG dictionary.\n# This makes it easy to experiment and adjust the pipeline's behavior.\nCONFIG = {\n    # General Settings\n    \"seed\": 42,                         # Random seed for reproducibility\n    \"num_labels\": 8,                    # Number of target classes (math topics)\n    \"base_model_name\": \"microsoft/deberta-v3-base\", # Transformer model identifier\n    \"max_length\": 384,                  # Max sequence length for Transformer input\n    \"output_dir_base\": \"./math_problem_classifier_final\", # Base directory for outputs\n    \"logging_steps\": 100,               # Frequency of logging during training\n    \"report_to\": \"none\",                # Reporting backend (e.g., \"wandb\", \"tensorboard\", or \"none\")\n\n    # Preprocessing Settings\n    \"transformer_preprocess_strategy\": \"linearize\", # \"linearize\" or \"special_tokens\"\n    \"math_special_tokens_list\": [       # List of special tokens for 'special_tokens' strategy\n        \"[MATH]\", \"[FRAC]\", \"[SQRT]\", \"[SUM]\", \"[INT]\", \"[LIM]\", \"[SUP]\", \"[SUB]\", \"[VEC]\",\n        \"[MAT]\", \"[EQ]\", \"[APPROX]\", \"[NEQ]\", \"[LT]\", \"[GT]\", \"[LEQ]\", \"[GEQ]\",\n        \"[TIMES]\", \"[DIV]\", \"[PM]\", \"[MP]\", \"[SIN]\", \"[COS]\", \"[TAN]\", \"[LOG]\", \"[LN]\"\n    ],\n    \"tfidf_max_features\": 500,          # Max features for TF-IDF (used with symbolic model)\n\n    # Stage 0: Contrastive Pre-training (Optional)\n    \"use_contrastive_pretraining\": False, # Enable/disable contrastive learning stage\n    \"contrastive_epochs\": 1,            # Epochs for contrastive learning\n    \"contrastive_batch_size\": 8,        # Batch size for contrastive learning\n    \"contrastive_warmup_steps_ratio\": 0.1, # Warmup ratio for contrastive learning\n    \"contrastive_output_path\": \"./math_problem_classifier_final/stage0_contrastive_model\",\n    \"contrastive_num_pairs_per_sample\": 1, # Positive pairs per anchor in contrastive learning\n\n    # Stage 1: Transformer Classification Fine-tuning\n    \"transformer_epochs\": 4,            # Epochs for Transformer fine-tuning\n    \"transformer_train_batch_size\": 8,  # Training batch size (per device)\n    \"transformer_eval_batch_size\": 16, # Evaluation batch size (per device)\n    \"transformer_base_learning_rate\": 2e-5, # Base learning rate for LLRD\n    \"transformer_weight_decay\": 0.01,   # Weight decay for Transformer\n    \"transformer_warmup_ratio\": 0.1,    # Warmup ratio for Transformer fine-tuning\n    \"transformer_grad_accum_steps\": 2,  # Gradient accumulation steps\n    \"transformer_fp16\": torch.cuda.is_available(), # Use mixed-precision training if CUDA available\n    \"transformer_early_stopping_patience\": 3, # Patience for early stopping\n    \"transformer_n_splits\": 5,          # Number of folds for cross-validation\n    \"llrd_decay_factor\": 0.90,          # Decay factor for LLRD\n    \"llrd_num_groups\": 6,               # Number of layer groups for LLRD\n    \"llrd_debug\": False,                # Enable verbose LLRD logging\n\n    # Stage 2: Symbolic Model (LightGBM)\n    \"use_symbolic_model\": True,         # Enable/disable symbolic model stage\n    \"lgbm_eval_split_size\": 0.15,       # Proportion of symbolic training data for LGBM early stopping eval\n    \"lgbm_params\": {                    # Parameters for LightGBM\n        'objective': 'multiclass',\n        'metric': 'multi_logloss',      # LGBM will use this for early stopping if eval_metric is not specified in fit\n        'num_class': 8,\n        'n_estimators': 1000,\n        'learning_rate': 0.05,\n        'feature_fraction': 0.8,\n        'bagging_fraction': 0.8,\n        'bagging_freq': 1,\n        'lambda_l1': 0.1,\n        'lambda_l2': 0.1,\n        'num_leaves': 31,\n        'verbose': -1,                  # Suppress LightGBM's own verbose output\n        'n_jobs': -1,\n        'seed': 42,\n        'boosting_type': 'gbdt',\n    },\n\n    # Ensembling Settings\n    \"ensemble_transformer_weight\": 0.75, # Weight for Transformer predictions in ensemble\n    \"ensemble_symbolic_weight\": 0.25,   # Weight for Symbolic model predictions\n\n    # Kaggle Specific Paths (adjust if running locally with different structure)\n    \"input_dir\": \"/kaggle/input/classification-of-math-problems-by-kasut-academy\",\n    \"train_csv\": \"train.csv\",\n    \"test_csv\": \"test.csv\",\n    \"submission_filename_prefix\": \"submission_final_ensemble\",\n}\n\n# Disable contrastive pre-training if sentence-transformers is not available.\nif not SENTENCE_TRANSFORMERS_AVAILABLE:\n    CONFIG[\"use_contrastive_pretraining\"] = False\n    print(\"INFO: Contrastive pre-training disabled as sentence-transformers is not available.\")\n\n# ==============================================================================\n# Setup Logging\n# ==============================================================================\n# Configure a logger for consistent and informative messages.\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n    datefmt='%Y-%m-%d %H:%M:%S'\n)\nlogger = logging.getLogger(\"MathProblemClassifier\")\nhf_logging.set_verbosity_warning() # Reduce verbosity from Hugging Face libraries.\nlogger.info(\"--- Logger Initialized ---\")\n\n# Disable Weights & Biases if not used, to prevent prompts.\nif CONFIG[\"report_to\"] == \"none\":\n    os.environ['WANDB_DISABLED'] = 'true'\n    logger.info(\"Weights & Biases logging explicitly disabled via WANDB_DISABLED=true\")\n\n# ==============================================================================\n# Seed for Reproducibility\n# ==============================================================================\n# Set random seeds for all relevant libraries to ensure reproducible results.\ndef seed_everything(seed):\n    np.random.seed(seed); random.seed(seed); os.environ['PYTHONHASHSEED'] = str(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n        # Disabling cudnn.benchmark can lead to more deterministic results but sometimes slower.\n        torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n    logger.info(f\"Seed set globally to {seed}\")\n\nseed_everything(CONFIG[\"seed\"])\n\n# ==============================================================================\n# Custom Callbacks & Utility Functions\n# ==============================================================================\n\nclass TrainingProgressCallback(TrainerCallback):\n    \"\"\"A custom Hugging Face TrainerCallback to log training progress (loss, LR, epoch, eval metrics).\"\"\"\n    def on_log(self, args, state, control, logs=None, **kwargs):\n        if state.is_world_process_zero and logs: # Only log on the main process\n            _log_str = \"\"\n            loss = logs.get('loss')\n            lr = logs.get('learning_rate')\n            if loss is not None: _log_str += f\"Loss={loss:.4f} | \"\n            if lr is not None: _log_str += f\"LR={lr:.2e} | \"\n            if 'epoch' in logs: _log_str += f\"Epoch={logs['epoch']:.2f} | \"\n            # Add any evaluation metrics logged\n            for k, v in logs.items():\n                if k.startswith(\"eval_\"): _log_str += f\"{k}={v:.4f} | \"\n            if _log_str: # Avoid logging empty strings\n                logger.info(f\"Logs Step {state.global_step}: {_log_str.rstrip(' | ')}\")\n\ndef load_dataframes():\n    \"\"\"Loads train and test datasets from CSV files specified in CONFIG.\"\"\"\n    logger.info(\"--- Starting Data Loading ---\")\n    start_time = time.time()\n    try:\n        train_csv_path = os.path.join(CONFIG['input_dir'], CONFIG['train_csv'])\n        test_csv_path = os.path.join(CONFIG['input_dir'], CONFIG['test_csv'])\n        train_df = pd.read_csv(train_csv_path)\n        test_df = pd.read_csv(test_csv_path)\n\n        # Basic validation of required columns\n        if 'Question' not in train_df.columns or 'label' not in train_df.columns:\n            raise KeyError(\"Train CSV must contain 'Question' and 'label' columns.\")\n        if 'Question' not in test_df.columns or 'id' not in test_df.columns:\n            raise KeyError(\"Test CSV must contain 'Question' and 'id' columns.\")\n\n        # Fill NA in 'Question' to prevent errors, assuming empty string is appropriate.\n        train_df['Question'] = train_df['Question'].fillna('')\n        test_df['Question'] = test_df['Question'].fillna('')\n        logger.info(f\"Train data shape: {train_df.shape}, Test data shape: {test_df.shape}\")\n        logger.info(f\"Train label distribution:\\n{train_df['label'].value_counts(normalize=True)}\")\n    except Exception as e:\n        logger.error(f\"Data loading error: {e}\"); raise\n    logger.info(f\"--- Data Loading finished in {time.time() - start_time:.2f}s ---\")\n    return train_df, test_df\n\ndef preprocess_math_text_for_transformer(text: str) -> str:\n    \"\"\"\n    Preprocesses mathematical text for Transformer models.\n    Supports two strategies:\n    - 'linearize': Converts LaTeX math expressions into more natural language.\n    - 'special_tokens': Replaces LaTeX commands with special tokens (e.g., [FRAC]).\n    \"\"\"\n    text = str(text) # Ensure input is a string\n    text = re.sub(r'\\s+', ' ', text).strip() # Normalize whitespace\n    strategy = CONFIG['transformer_preprocess_strategy']\n\n    if strategy == 'linearize':\n        # Replace common LaTeX structures with textual descriptions.\n        replacements = [\n            (r\"\\\\frac\\{([^}]+)\\}\\{([^}]+)\\}\", r\" fraction \\1 over \\2 \"), (r\"\\\\sqrt\\{([^}]+)\\}\", r\" square root of \\1 \"),\n            (r\"\\\\sum_\\{([^}]+)\\}\\^\\{([^}]+)\\}\", r\" summation from \\1 to \\2 of \"), (r\"\\\\sum\", \" summation \"),\n            (r\"\\\\int_\\{([^}]+)\\}\\^\\{([^}]+)\\}\", r\" integral from \\1 to \\2 of \"), (r\"\\\\int\", \" integral \"),\n            (r\"\\\\lim_\\{([^}]+)\\}\", r\" limit as approaches \\1 of \"), (r\"\\\\lim\", \" limit \"),\n            (r\"\\^\\{([^}]+)\\}\", r\" superscript \\1 \"), (r\"_\\{([^}]+)\\}\", r\" subscript \\1 \"),\n            (r\"\\\\vec\\{([^}]+)\\}\", r\" vector \\1 \"), (r\"\\\\mathbf\\{([^}]+)\\}\", r\" matrix \\1 \"),\n            (r\"\\\\begin\\{pmatrix\\}.*?\\\\end\\{pmatrix\\}\", \" matrix expression \"),\n            (r\"\\\\begin\\{bmatrix\\}.*?\\\\end\\{bmatrix\\}\", \" matrix expression \"),\n            (r\"\\\\begin\\{matrix\\}.*?\\\\end\\{matrix\\}\", \" matrix expression \"),\n            (r\"\\\\[Ss]in\", \" sine \"), (r\"\\\\[Cc]os\", \" cosine \"), (r\"\\\\[Tt]an\", \" tangent \"),\n            (r\"\\\\[Ll]og\", \" log \"), (r\"\\\\[Ll]n\", \" natural log \"),\n            (r\"\\\\approx\", \" approximately equal \"), (r\"\\\\neq\", \" not equal \"),\n            (r\"\\\\leq\", \" less than or equal \"), (r\"\\\\geq\", \" greater than or equal \"),\n            (r\"\\\\times\", \" times \"), (r\"\\\\cdot\", \" times \"), (r\"\\\\div\", \" divided by \"),\n            (r\"\\\\pm\", \" plus minus \"), (r\"\\\\mp\", \" minus plus \"),\n            (r\"<\", \" less than \"), (r\">\", \" greater than \"), (r\"=\", \" equals \"),\n            (r\"\\$(.*?)\\$\", r\" math expression \\1 math expression \"), # Preserve content within $...$\n            (r\"\\{\", \"\"), (r\"\\}\", \"\"), (r\"\\\\\", \" \") # General cleanup for remaining backslashes\n        ]\n        for pattern, replacement in replacements:\n            try: text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n            except Exception as e: logger.warning(f\"Regex error (linearize) pattern {pattern}: {e}\")\n\n    elif strategy == 'special_tokens':\n        # Replace LaTeX with predefined special tokens (see CONFIG['math_special_tokens_list'])\n        replacements = [\n            (r\"\\\\frac\\{.*?\\}\\{.*?\\}\", \" [FRAC] \"), (r\"\\\\sqrt\\{.*?\\}\", \" [SQRT] \"), (r\"\\\\sum\", \" [SUM] \"),\n            (r\"\\\\int\", \" [INT] \"), (r\"\\\\lim\", \" [LIM] \"), (r\"\\^\\{.*?\\}\", \" [SUP] \"), (r\"_\\{.*?\\}\", \" [SUB] \"),\n            (r\"\\\\vec\\{.*?\\}\", \" [VEC] \"), (r\"\\\\mathbf\\{.*?\\}\", \" [MAT] \"),\n            (r\"\\\\begin\\{(?:pmatrix|bmatrix|matrix)\\}.*?\\\\end\\{(?:pmatrix|bmatrix|matrix)\\}\", \" [MAT] \"),\n            (r\"=\", \" [EQ] \"), (r\"\\\\approx\", \" [APPROX] \"), (r\"\\\\neq\", \" [NEQ] \"),\n            (r\"\\\\leq\", \" [LEQ] \"), (r\"<\", \" [LT] \"), (r\"\\\\geq\", \" [GEQ] \"), (r\">\", \" [GT] \"),\n            (r\"\\\\times\", \" [TIMES] \"), (r\"\\\\cdot\", \" [TIMES] \"), (r\"\\\\div\", \" [DIV] \"),\n            (r\"\\\\pm\", \" [PM] \"), (r\"\\\\mp\", \" [MP] \"), (r\"\\\\[Ss]in\", \" [SIN] \"),\n            (r\"\\\\[Cc]os\", \" [COS] \"), (r\"\\\\[Tt]an\", \" [TAN] \"), (r\"\\\\[Ll]og\", \" [LOG] \"),\n            (r\"\\\\[Ll]n\", \" [LN] \"),\n            (r\"\\$(.*?)\\$\", r\" [MATH] \\1 [MATH] \"), # Mark math regions\n            (r\"\\{|\\}\", \"\"), (r\"\\\\\", \" \") # General cleanup\n        ]\n        for pattern, replacement in replacements:\n             try: text = re.sub(pattern, replacement, text, flags=re.IGNORECASE)\n             except Exception as e: logger.warning(f\"Regex error (special_tokens) pattern {pattern}: {e}\")\n    else: # Minimal preprocessing if no specific strategy is chosen\n        text = str(text).strip()\n        text = re.sub(r'\\s+', ' ', text)\n\n    text = re.sub(r'\\s+', ' ', text).strip() # Final whitespace normalization\n    return text\n\ndef extract_symbolic_features(text_series: pd.Series) -> pd.DataFrame:\n    \"\"\"\n    Engineers a set of symbolic features from raw mathematical text.\n    These features include counts of LaTeX commands, keywords, delimiters,\n    and basic text statistics, intended for a model like LightGBM.\n    \"\"\"\n    logger.info(\"Extracting symbolic features...\")\n    all_features_list = []\n    for text_content in text_series:\n        text_lower = str(text_content).lower() # Normalize to lowercase for keyword matching\n        current_features = {}\n\n        # Counts of math delimiters\n        current_features['n_math_dollar'] = text_lower.count('$')\n        current_features['n_math_display'] = len(re.findall(r'\\$\\$', text_lower))\n\n        # Counts of common LaTeX commands and structures\n        latex_commands_map = {\n            'n_frac': r'\\\\frac', 'n_int': r'\\\\int', 'n_sum': r'\\\\sum', 'n_lim': r'\\\\lim',\n            'n_matrix': r'\\\\begin\\{(?:matrix|pmatrix|bmatrix)\\}|\\\\mathbf', 'n_vec': r'\\\\vec',\n            'n_sqrt': r'\\\\sqrt', 'n_partial': r'\\\\partial',\n            'n_sin': r'\\\\[sS]in', 'n_cos': r'\\\\[cC]os', 'n_tan': r'\\\\[tT]an',\n            'n_log': r'\\\\[lL]og', 'n_ln': r'\\\\[lL]n',\n            'n_greek': r'\\\\(?:alpha|beta|gamma|delta|epsilon|zeta|eta|theta|iota|kappa|lambda|mu|nu|xi|omicron|pi|rho|sigma|tau|upsilon|phi|chi|psi|omega)',\n            'n_mathrm': r'\\\\mathrm', 'n_mathcal': r'\\\\mathcal', 'n_mathbb': r'\\\\mathbb',\n            'n_operatorname': r'\\\\operatorname'\n        }\n        for name, pattern in latex_commands_map.items():\n            # Use original text (not lowercased) if case sensitivity matters for some LaTeX commands\n            current_features[name] = len(re.findall(pattern, str(text_content)))\n\n        current_features['n_pow'] = str(text_content).count('^')\n        current_features['n_sub'] = str(text_content).count('_')\n        current_features['n_prime'] = str(text_content).count(\"'\")\n        current_features['n_equals'] = str(text_content).count(\"=\")\n        current_features['n_plus_minus'] = len(re.findall(r'\\\\pm', str(text_content)))\n        current_features['n_times_cdot'] = len(re.findall(r'\\\\times|\\\\cdot', str(text_content)))\n        current_features['n_div'] = len(re.findall(r'\\\\div', str(text_content)))\n        current_features['n_braces'] = str(text_content).count(\"{\") + str(text_content).count(\"}\")\n        current_features['n_brackets'] = str(text_content).count(\"[\") + str(text_content).count(\"]\")\n        current_features['n_parentheses'] = str(text_content).count(\"(\") + str(text_content).count(\")\")\n\n        # Counts of mathematical and logical keywords\n        keywords_list = [\n            'prove', 'solve', 'find', 'calculate', 'compute', 'determine', 'show', 'express',\n            'integral', 'derivative', 'limit', 'matrix', 'vector', 'eigenvalue', 'eigenvector',\n            'algebra', 'calculus', 'probability', 'statistics', 'geometry', 'topology', 'combinatorics', 'logic',\n            'linear', 'differential', 'equation', 'function', 'set', 'space', 'group', 'ring', 'field',\n            'theorem', 'proof', 'lemma', 'corollary', 'series', 'sequence', 'graph', 'polynomial',\n            'real', 'complex', 'number', 'integer', 'rational', 'irrational',\n            'if and only if', 'such that', 'let', 'assume', 'suppose', 'given', 'then', 'hence', 'thus'\n        ]\n        for kw in keywords_list:\n            # Use word boundaries (\\b) for precise keyword matching\n            current_features[f'kw_{kw.replace(\" \", \"_\")}'] = len(re.findall(r'\\b' + re.escape(kw) + r'\\b', text_lower))\n\n        # Basic text statistics\n        current_features['text_len'] = len(str(text_content))\n        current_features['n_words'] = len(str(text_content).split())\n        current_features['n_unique_words'] = len(set(text_lower.split()))\n        current_features['n_digits'] = sum(c.isdigit() for c in str(text_content))\n        current_features['n_uppercase'] = sum(c.isupper() for c in str(text_content))\n\n        # Ratios, adding a small epsilon to avoid division by zero\n        epsilon = 1e-6\n        current_features['ratio_math_delimiters_len'] = (current_features['n_math_dollar'] + current_features['n_math_display']*2) / (current_features['text_len'] + epsilon)\n        total_latex_symbols = sum(v for k, v in current_features.items() if k.startswith('n_') and k not in ['n_math_dollar', 'n_math_display', 'n_words', 'n_unique_words', 'n_digits', 'n_uppercase', 'text_len'])\n        current_features['ratio_latex_symbols_len'] = total_latex_symbols / (current_features['text_len'] + epsilon)\n        total_keywords = sum(v for k, v in current_features.items() if k.startswith('kw_'))\n        current_features['ratio_keywords_words'] = total_keywords / (current_features['n_words'] + epsilon)\n        current_features['ratio_digits_len'] = current_features['n_digits'] / (current_features['text_len'] + epsilon)\n        current_features['avg_word_len'] = sum(len(w) for w in text_lower.split()) / (current_features['n_words'] + epsilon)\n\n        all_features_list.append(current_features)\n    \n    df_features = pd.DataFrame(all_features_list)\n    return df_features\n\ndef combine_symbolic_and_tfidf(\n    df_sym_features: pd.DataFrame, \n    text_series_raw: pd.Series, \n    fit_vectorizer: bool = True, \n    vectorizer: Optional[TfidfVectorizer] = None\n) -> Tuple[pd.DataFrame, TfidfVectorizer]:\n    \"\"\"\n    Combines symbolic features with TF-IDF features.\n    Fits a new TF-IDF vectorizer if `fit_vectorizer` is True, otherwise uses the provided one.\n    \"\"\"\n    logger.info(\"Combining symbolic features with TF-IDF...\")\n    if fit_vectorizer:\n        # Initialize TF-IDF vectorizer with parameters optimized for math text\n        tfidf_vectorizer = TfidfVectorizer(\n            max_features=CONFIG['tfidf_max_features'],\n            token_pattern=r'(?u)\\b\\w+\\b|\\$|\\$\\$|\\\\(?:[a-zA-Z]+|\\W)', # Captures words, delimiters, LaTeX\n            ngram_range=(1,2), # Include unigrams and bigrams\n            min_df=3,          # Ignore terms with document frequency lower than 3\n            max_df=0.9         # Ignore terms with document frequency higher than 0.9\n        )\n        X_tfidf = tfidf_vectorizer.fit_transform(text_series_raw.astype(str))\n    else:\n        if vectorizer is None: raise ValueError(\"Vectorizer must be provided if not fitting.\")\n        tfidf_vectorizer = vectorizer\n        X_tfidf = tfidf_vectorizer.transform(text_series_raw.astype(str))\n\n    # Convert TF-IDF sparse matrix to DataFrame and prefix columns\n    df_tfidf = pd.DataFrame(X_tfidf.toarray(), index=df_sym_features.index).add_prefix('tfidf_')\n    \n    # Concatenate symbolic and TF-IDF features\n    X_combined = pd.concat([df_sym_features, df_tfidf], axis=1)\n    \n    # Ensure column names are strings (requirement for some models like LightGBM)\n    X_combined.columns = [str(col) for col in X_combined.columns]\n    logger.info(f\"Combined symbolic and TF-IDF features shape: {X_combined.shape}\")\n    return X_combined, tfidf_vectorizer\n\ndef tokenize_function_transformer(examples: Dict, tokenizer: AutoTokenizer) -> Dict:\n    \"\"\"Tokenizes text data for the Transformer model using the provided tokenizer.\"\"\"\n    # Assumes 'processed_text_transformer' column exists in examples\n    return tokenizer(\n        examples[\"processed_text_transformer\"],\n        truncation=True,\n        max_length=CONFIG[\"max_length\"],\n        padding=False # Padding will be handled by DataCollatorWithPadding\n    )\n\ndef compute_f1_micro(eval_pred: Tuple) -> Dict[str, float]:\n    \"\"\"Computes F1-micro score, the official competition metric.\"\"\"\n    logits, labels = eval_pred\n    if isinstance(logits, tuple): logits = logits[0] # Handle models outputting tuples\n    \n    # Handle potential NaNs or Infs in logits if any numerical instability occurs\n    if not np.all(np.isfinite(logits)): logits = np.nan_to_num(logits)\n    \n    predictions = np.argmax(logits, axis=1)\n    \n    # Ensure labels are integers for F1 score calculation\n    try: labels = labels.astype(int)\n    except ValueError: return {\"f1_micro\": 0.0} # Return 0 if labels can't be cast\n    \n    # Basic shape check\n    if labels.shape != predictions.shape: return {\"f1_micro\": 0.0}\n    \n    try:\n        f1 = f1_score(labels, predictions, average=\"micro\", zero_division=0)\n    except Exception: # Catch any other sklearn errors\n        return {\"f1_micro\": 0.0}\n    return {\"f1_micro\": f1}\n\ndef get_llrd_parameter_groups(\n    model: nn.Module, \n    base_lr: float, \n    decay_factor: float, \n    weight_decay: float, \n    num_groups: int\n) -> List[Dict]:\n    \"\"\"\n    Creates parameter groups for Layer-wise Learning Rate Decay (LLRD).\n    Assigns different learning rates to different parts of the model (embeddings, encoder layers, head).\n    Lower layers (embeddings, early encoder layers) get smaller LRs.\n    \"\"\"\n    logger.info(f\"Creating LLRD parameter groups: BaseLR={base_lr:.2e}, DecayFactor={decay_factor}, WD={weight_decay}, NumGroups={num_groups}\")\n    \n    # Dynamically find the base model attribute (e.g., 'deberta', 'roberta', 'bert')\n    model_type_str = model.config.model_type.split('-')[0]\n    base_model_attribute = getattr(model, model_type_str, None)\n    if base_model_attribute is None: # Fallback strategy\n        base_model_attribute = getattr(model, 'base_model', None)\n        if base_model_attribute is None:\n            known_attributes = ['bert', 'roberta', 'deberta', 'electra', 'albert']\n            for attr_name in known_attributes:\n                if hasattr(model, attr_name):\n                    base_model_attribute = getattr(model, attr_name)\n                    logger.warning(f\"LLRD: Using found base model attribute '{attr_name}'.\")\n                    break\n            if base_model_attribute is None:\n                raise ValueError(f\"Cannot find base model attribute for LLRD. Model attributes: {dir(model)}\")\n\n    # Determine the number of encoder layers\n    encoder_layers_obj = getattr(base_model_attribute, \"encoder\", None)\n    if encoder_layers_obj is None and hasattr(base_model_attribute, \"layer\"): # E.g. Electra\n         encoder_layers_obj = base_model_attribute\n    \n    if encoder_layers_obj is None or not hasattr(encoder_layers_obj, \"layer\"):\n        num_encoder_layers = model.config.num_hidden_layers if hasattr(model.config, 'num_hidden_layers') else 12\n        logger.warning(f\"LLRD: Cannot find 'encoder.layer'. Assuming {num_encoder_layers} layers from config.\")\n    else:\n        num_encoder_layers = len(encoder_layers_obj.layer)\n    logger.info(f\"LLRD: Detected {num_encoder_layers} encoder layers for grouping.\")\n    \n    # Calculate layers per group for encoder layers\n    num_encoder_groups = num_groups - 2 # Exclude embeddings and head groups\n    if num_encoder_groups <= 0: # Handle cases with few groups\n        layers_per_group = num_encoder_layers\n        num_encoder_groups = 1 if num_encoder_layers > 0 else 0\n    else:\n        layers_per_group = num_encoder_layers // num_encoder_groups\n    if layers_per_group == 0 and num_encoder_layers > 0: layers_per_group = 1 # Avoid zero layers per group\n    logger.info(f\"LLRD: Calculated ~{layers_per_group} layers per group for {num_encoder_groups} encoder groups.\")\n\n    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\"] # Parameters excluded from weight decay\n    optimizer_grouped_parameters = []\n    assigned_param_ids = set() # Track parameters already assigned to a group\n\n    # Group 0: Embeddings (lowest learning rate)\n    lr_embeddings = base_lr * (decay_factor ** (num_groups - 1))\n    embeddings_params_wd, embeddings_params_no_wd = [], []\n    embedding_keywords = [\"embeddings\", \"Embedding\"] # Keywords to identify embedding layers\n    for n, p in base_model_attribute.named_parameters():\n        is_embedding = any(keyword in n.lower() for keyword in embedding_keywords)\n        is_not_in_encoder = \"encoder.layer\" not in n # Avoid double-counting if an encoder layer has 'embeddings' in name\n        if is_embedding and is_not_in_encoder and p.requires_grad:\n            if any(nd in n for nd in no_decay): embeddings_params_no_wd.append(p)\n            else: embeddings_params_wd.append(p)\n            assigned_param_ids.add(id(p))\n            if CONFIG[\"llrd_debug\"]: logger.debug(f\"  LLRD Group Embeddings: {n} (LR: {lr_embeddings:.2e})\")\n    optimizer_grouped_parameters.extend([\n        {'params': embeddings_params_wd, 'lr': lr_embeddings, 'weight_decay': weight_decay},\n        {'params': embeddings_params_no_wd, 'lr': lr_embeddings, 'weight_decay': 0.0},])\n    logger.info(f\"LLRD Group 0 (Embeddings): LR={lr_embeddings:.2e}, ParamsWD={len(embeddings_params_wd)}, ParamsNoWD={len(embeddings_params_no_wd)}\")\n\n    # Groups 1 to num_groups-2: Encoder layers\n    for i in range(num_encoder_groups):\n        group_idx_display = i + 1 # For logging\n        lr_group = base_lr * (decay_factor ** (num_groups - 1 - group_idx_display))\n        start_layer = i * layers_per_group\n        end_layer = start_layer + layers_per_group\n        if i == num_encoder_groups - 1: end_layer = num_encoder_layers # Last group takes all remaining\n\n        layer_params_wd, layer_params_no_wd = [], []\n        for layer_idx in range(start_layer, end_layer):\n            if layer_idx >= num_encoder_layers: break\n            layer_name_prefixes = [f\"encoder.layer.{layer_idx}.\", f\"layer.{layer_idx}.\"] # Common naming patterns\n            for n, p in base_model_attribute.named_parameters():\n                if any(n.startswith(prefix) for prefix in layer_name_prefixes) and \\\n                   p.requires_grad and id(p) not in assigned_param_ids:\n                    if any(nd in n for nd in no_decay): layer_params_no_wd.append(p)\n                    else: layer_params_wd.append(p)\n                    assigned_param_ids.add(id(p))\n                    if CONFIG[\"llrd_debug\"]: logger.debug(f\"  LLRD Group {group_idx_display} (Layer {layer_idx}): {n} (LR: {lr_group:.2e})\")\n        optimizer_grouped_parameters.extend([\n            {'params': layer_params_wd, 'lr': lr_group, 'weight_decay': weight_decay},\n            {'params': layer_params_no_wd, 'lr': lr_group, 'weight_decay': 0.0},])\n        logger.info(f\"LLRD Group {group_idx_display} (Layers {start_layer}-{end_layer-1}): LR={lr_group:.2e}, ParamsWD={len(layer_params_wd)}, ParamsNoWD={len(layer_params_no_wd)}\")\n\n    # Group num_groups-1: Pooler and Classifier Head (highest learning rate)\n    lr_head = base_lr\n    head_params_wd, head_params_no_wd = [], []\n    # Assign any remaining trainable parameters (typically head/pooler) to this group\n    for n, p in model.named_parameters():\n         if p.requires_grad and id(p) not in assigned_param_ids:\n             if any(nd in n for nd in no_decay): head_params_no_wd.append(p)\n             else: head_params_wd.append(p)\n             if CONFIG[\"llrd_debug\"]: logger.debug(f\"  LLRD Group Head/Other: {n} (LR: {lr_head:.2e})\")\n    optimizer_grouped_parameters.extend([\n        {'params': head_params_wd, 'lr': lr_head, 'weight_decay': weight_decay},\n        {'params': head_params_no_wd, 'lr': lr_head, 'weight_decay': 0.0},])\n    logger.info(f\"LLRD Group {num_groups-1} (Head/Pooler/Other): LR={lr_head:.2e}, ParamsWD={len(head_params_wd)}, ParamsNoWD={len(head_params_no_wd)}\")\n\n    # Sanity check for parameter assignment\n    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    assigned_params_count = sum(p.numel() for group in optimizer_grouped_parameters for p_item in group[\"params\"] if isinstance(p_item, torch.Tensor))\n    if total_trainable_params != assigned_params_count:\n        logger.warning(f\"LLRD Parameter Assignment Mismatch: Total Trainable={total_trainable_params}, Total Assigned={assigned_params_count}. Diff={total_trainable_params - assigned_params_count}\")\n\n    return optimizer_grouped_parameters\n\nclass LLRDTrainer(Trainer):\n    \"\"\"\n    Custom Hugging Face Trainer that implements Layer-wise Learning Rate Decay (LLRD).\n    It overrides `create_optimizer_and_scheduler` to set up parameter groups\n    with different learning rates.\n    \"\"\"\n    def create_optimizer_and_scheduler(self, num_training_steps: int):\n        if self.optimizer is None:\n            logger.info(\"--- LLRDTrainer: Creating custom LLRD optimizer and scheduler ---\")\n            param_groups = get_llrd_parameter_groups(\n                self.model,\n                base_lr=self.args.learning_rate, # This is CONFIG['transformer_base_learning_rate']\n                decay_factor=CONFIG['llrd_decay_factor'],\n                weight_decay=self.args.weight_decay, # This is CONFIG['transformer_weight_decay']\n                num_groups=CONFIG['llrd_num_groups']\n            )\n            self.optimizer = AdamW(\n                param_groups,\n                lr=self.args.learning_rate, # Base LR, overridden by group LRs\n                eps=self.args.adam_epsilon,\n                betas=(self.args.adam_beta1, self.args.adam_beta2)\n            )\n            logger.info(\"Custom AdamW optimizer for LLRD created inside LLRDTrainer.\")\n        else:\n            logger.info(\"Optimizer already exists in LLRDTrainer, not recreating.\")\n\n        if self.lr_scheduler is None:\n            self.lr_scheduler = get_scheduler(\n                name=self.args.lr_scheduler_type, # e.g., 'linear', 'cosine'\n                optimizer=self.optimizer,\n                num_warmup_steps=self.args.get_warmup_steps(num_training_steps),\n                num_training_steps=num_training_steps\n            )\n            logger.info(f\"Scheduler created inside LLRDTrainer: Type={self.args.lr_scheduler_type}, WarmupSteps={self.args.get_warmup_steps(num_training_steps)}, TotalSteps={num_training_steps}\")\n        else:\n            logger.info(\"Scheduler already exists in LLRDTrainer, not recreating.\")\n        # In newer HF versions, this method should not return. It sets attributes.\n\n\n# ==============================================================================\n# Main Execution Pipeline\n# ==============================================================================\ndef main():\n    \"\"\"Main function to run the entire math problem classification pipeline.\"\"\"\n    \n    # Create base output directory for storing models, logs, and submissions.\n    os.makedirs(CONFIG[\"output_dir_base\"], exist_ok=True)\n    \n    # --- Load Data ---\n    train_df, test_df = load_dataframes()\n\n    # --- Prepare Text for Transformer Model ---\n    # This processed text will be used for both contrastive pre-training (if enabled)\n    # and the main classification fine-tuning.\n    logger.info(f\"Preprocessing text for Transformer using strategy: {CONFIG['transformer_preprocess_strategy']}\")\n    train_df['processed_text_transformer'] = train_df['Question'].apply(preprocess_math_text_for_transformer)\n    test_df['processed_text_transformer'] = test_df['Question'].apply(preprocess_math_text_for_transformer)\n\n    # --- STAGE 0: CONTRASTIVE PRE-TRAINING (Optional) ---\n    # This stage aims to adapt the Transformer's embeddings to the mathematical domain\n    # before fine-tuning on the specific classification task.\n    transformer_model_path_for_finetuning = CONFIG['base_model_name'] # Default to base model\n    if CONFIG['use_contrastive_pretraining']:\n        if not SENTENCE_TRANSFORMERS_AVAILABLE:\n            logger.warning(\"Skipping contrastive pre-training: sentence-transformers library not installed.\")\n        else:\n            logger.info(\"\\n===== Starting Stage 0: Contrastive Fine-tuning =====\")\n            os.makedirs(CONFIG['contrastive_output_path'], exist_ok=True)\n            \n            # Prepare positive pairs for contrastive learning (MultipleNegativesRankingLoss).\n            # Pairs are formed from problems of the same category.\n            train_data_grouped_ct = train_df.groupby('label')\n            contrastive_train_samples = []\n            for _, group_df in train_data_grouped_ct:\n                texts_in_group = group_df['processed_text_transformer'].tolist()\n                if len(texts_in_group) < 2: continue # Need at least two samples to form a pair\n                for i in range(len(texts_in_group)):\n                    anchor_text = texts_in_group[i]\n                    # Create specified number of positive pairs for each anchor\n                    for _ in range(CONFIG['contrastive_num_pairs_per_sample']):\n                        possible_positives = texts_in_group[:i] + texts_in_group[i+1:]\n                        if not possible_positives: continue\n                        positive_text = random.choice(possible_positives)\n                        contrastive_train_samples.append(InputExample(texts=[anchor_text, positive_text]))\n            \n            logger.info(f\"Created {len(contrastive_train_samples)} positive pairs for contrastive training.\")\n\n            if contrastive_train_samples:\n                # Initialize SentenceTransformer model\n                word_embedding_model = models.Transformer(CONFIG['base_model_name'], max_seq_length=CONFIG['max_length'])\n                pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n                model_st = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n                \n                contrastive_loss_fn = losses.MultipleNegativesRankingLoss(model=model_st)\n                \n                contrastive_dataloader_st = TorchDataLoader(\n                    contrastive_train_samples, shuffle=True, batch_size=CONFIG['contrastive_batch_size']\n                )\n                \n                num_train_steps_ct = len(contrastive_dataloader_st) * CONFIG['contrastive_epochs']\n                num_warmup_steps_ct = int(num_train_steps_ct * CONFIG['contrastive_warmup_steps_ratio'])\n\n                logger.info(f\"Starting contrastive training for {CONFIG['contrastive_epochs']} epochs...\")\n                model_st.fit(\n                    train_objectives=[(contrastive_dataloader_st, contrastive_loss_fn)],\n                    epochs=CONFIG['contrastive_epochs'], warmup_steps=num_warmup_steps_ct,\n                    output_path=CONFIG['contrastive_output_path'], show_progress_bar=True,\n                    checkpoint_save_steps=None, checkpoint_path=None, save_best_model=False # Save final model\n                )\n                transformer_model_path_for_finetuning = CONFIG['contrastive_output_path'] # Use this pre-trained model\n                logger.info(f\"Stage 0 (Contrastive) finished. Model saved to: {transformer_model_path_for_finetuning}\")\n                del model_st, contrastive_dataloader_st, contrastive_train_samples, train_data_grouped_ct, word_embedding_model, pooling_model\n            else:\n                logger.warning(\"No contrastive pairs generated. Skipping contrastive pre-training.\")\n            gc.collect(); torch.cuda.empty_cache() # Clean up memory\n\n    # --- Initialize Tokenizer for Transformer ---\n    # The tokenizer should generally match the original base model, even after SBERT pre-training,\n    # as SBERT's `models.Transformer` loads the tokenizer from the base model name.\n    tokenizer_load_path = CONFIG['base_model_name']\n    logger.info(f\"Loading Tokenizer for Transformer from: {tokenizer_load_path}\")\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_load_path)\n\n    # Add special math tokens if that strategy is chosen\n    math_special_tokens_to_add = []\n    if CONFIG['transformer_preprocess_strategy'] == 'special_tokens':\n        math_special_tokens_to_add = CONFIG['math_special_tokens_list']\n        num_added_toks = tokenizer.add_tokens(math_special_tokens_to_add, special_tokens=True)\n        logger.info(f\"Added {num_added_toks} special math tokens to tokenizer vocabulary.\")\n\n    # --- Tokenize Data for Transformer using Hugging Face Datasets ---\n    logger.info(\"Tokenizing data for Transformer classification stage...\")\n    train_hf_ds_tf = HFDataset.from_pandas(train_df[['processed_text_transformer', 'label']])\n    test_hf_ds_tf = HFDataset.from_pandas(test_df[['processed_text_transformer']]) # Test set has no labels\n\n    # Apply tokenization\n    tokenized_train_ds_tf = train_hf_ds_tf.map(\n        tokenize_function_transformer, batched=True, fn_kwargs={\"tokenizer\": tokenizer},\n        remove_columns=['processed_text_transformer'] # Remove original text column after tokenization\n    )\n    tokenized_test_ds_tf = test_hf_ds_tf.map(\n        tokenize_function_transformer, batched=True, fn_kwargs={\"tokenizer\": tokenizer},\n        remove_columns=['processed_text_transformer']\n    )\n    tokenized_train_ds_tf.set_format(\"torch\") # Ensure PyTorch format for Trainer\n\n    # Data collator handles dynamic padding of batches\n    data_collator_tf = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    # --- STAGE 1: TRANSFORMER CLASSIFICATION FINE-TUNING (K-Fold with LLRD) ---\n    logger.info(f\"\\n===== Starting Stage 1: {CONFIG['transformer_n_splits']}-Fold Transformer Fine-tuning =====\")\n    skf_tf = StratifiedKFold(n_splits=CONFIG['transformer_n_splits'], shuffle=True, random_state=CONFIG['seed'])\n    \n    # Initialize arrays to store Out-Of-Fold (OOF) predictions and test predictions\n    oof_preds_transformer_logits = np.zeros((len(train_df), CONFIG['num_labels']))\n    test_preds_transformer_logits_folds = np.zeros((len(test_df), CONFIG['transformer_n_splits'], CONFIG['num_labels']))\n    fold_transformer_val_scores = [] # Store validation F1 score for each fold\n\n    # K-Fold Cross-Validation Loop\n    for fold_num, (train_idx, val_idx) in enumerate(skf_tf.split(X=np.zeros(len(train_df)), y=train_df['label'])):\n        fold_start_time = time.time()\n        current_fold_seed = CONFIG['seed'] + fold_num # Vary seed per fold for diversity if desired\n        seed_everything(current_fold_seed)\n        output_dir_fold_tf = os.path.join(CONFIG['output_dir_base'], f\"stage1_transformer_fold_{fold_num}\")\n        os.makedirs(output_dir_fold_tf, exist_ok=True)\n\n        logger.info(f\"\\n===== Transformer Fold {fold_num+1}/{CONFIG['transformer_n_splits']} =====\")\n        logger.info(f\"Seed for this fold: {current_fold_seed}, Output directory: {output_dir_fold_tf}\")\n\n        # Select data for the current fold\n        train_fold_dataset_tf = tokenized_train_ds_tf.select(train_idx)\n        val_fold_dataset_tf = tokenized_train_ds_tf.select(val_idx)\n        logger.info(f\"Fold {fold_num+1}: Train size: {len(train_fold_dataset_tf)}, Validation size: {len(val_fold_dataset_tf)}\")\n\n        # Load model configuration and model for sequence classification\n        model_config_tf_fold = AutoConfig.from_pretrained(transformer_model_path_for_finetuning, num_labels=CONFIG['num_labels'])\n        model_tf_fold = AutoModelForSequenceClassification.from_pretrained(transformer_model_path_for_finetuning, config=model_config_tf_fold)\n        \n        # Resize token embeddings if special tokens were added\n        if math_special_tokens_to_add:\n            model_tf_fold.resize_token_embeddings(len(tokenizer))\n            logger.info(\"Resized token embeddings for Transformer model in this fold.\")\n\n        # Define TrainingArguments for the Hugging Face Trainer\n        training_args_tf_fold = TrainingArguments(\n            output_dir=output_dir_fold_tf,\n            num_train_epochs=CONFIG['transformer_epochs'],\n            per_device_train_batch_size=CONFIG['transformer_train_batch_size'],\n            per_device_eval_batch_size=CONFIG['transformer_eval_batch_size'],\n            gradient_accumulation_steps=CONFIG['transformer_grad_accum_steps'],\n            learning_rate=CONFIG['transformer_base_learning_rate'], # Base LR for LLRD\n            weight_decay=CONFIG['transformer_weight_decay'],\n            warmup_ratio=CONFIG['transformer_warmup_ratio'],\n            logging_dir=os.path.join(output_dir_fold_tf, 'logs'),\n            logging_strategy=\"steps\", logging_steps=CONFIG['logging_steps'],\n            eval_strategy=\"epoch\", save_strategy=\"epoch\", # Evaluate and save at end of each epoch\n            load_best_model_at_end=True, metric_for_best_model=\"f1_micro\", greater_is_better=True,\n            save_total_limit=1, # Save only the best model checkpoint\n            report_to=CONFIG[\"report_to\"],\n            fp16=CONFIG['transformer_fp16'], # Mixed precision training\n            seed=current_fold_seed,\n            dataloader_num_workers=2, # Adjust based on CPU cores and system\n            dataloader_pin_memory=True,\n            remove_unused_columns=True # Important for Hugging Face datasets\n        )\n\n        # Initialize LLRDTrainer\n        trainer_tf_fold = LLRDTrainer(\n            model=model_tf_fold, args=training_args_tf_fold,\n            train_dataset=train_fold_dataset_tf, eval_dataset=val_fold_dataset_tf,\n            tokenizer=tokenizer, data_collator=data_collator_tf,\n            compute_metrics=compute_f1_micro,\n            callbacks=[\n                EarlyStoppingCallback(early_stopping_patience=CONFIG['transformer_early_stopping_patience']),\n                TrainingProgressCallback()\n            ]\n        )\n        \n        logger.info(f\"Starting Transformer training for fold {fold_num+1}...\")\n        trainer_tf_fold.train() # Start training\n        \n        logger.info(f\"Evaluating best Transformer model on validation set for fold {fold_num+1}...\")\n        eval_results_tf_fold = trainer_tf_fold.evaluate(val_fold_dataset_tf)\n        fold_val_score_tf = eval_results_tf_fold.get('eval_f1_micro', 0.0)\n        fold_transformer_val_scores.append(fold_val_score_tf)\n        logger.info(f\"Fold {fold_num+1} Transformer Val F1-micro: {fold_val_score_tf:.5f}, Loss: {eval_results_tf_fold.get('eval_loss', 'N/A'):.4f}\")\n\n        logger.info(f\"Predicting OOF (Out-Of-Fold) with Transformer for fold {fold_num+1}...\")\n        oof_outputs_tf_fold = trainer_tf_fold.predict(val_fold_dataset_tf)\n        oof_preds_transformer_logits[val_idx] = oof_outputs_tf_fold.predictions # Store raw logits\n\n        logger.info(f\"Predicting on Test set with Transformer for fold {fold_num+1}...\")\n        test_outputs_tf_fold = trainer_tf_fold.predict(tokenized_test_ds_tf)\n        test_preds_transformer_logits_folds[:, fold_num, :] = test_outputs_tf_fold.predictions # Store raw logits\n        \n        logger.info(f\"Cleaning up Transformer fold {fold_num+1} resources...\")\n        del model_tf_fold, trainer_tf_fold, train_fold_dataset_tf, val_fold_dataset_tf\n        del training_args_tf_fold, oof_outputs_tf_fold, test_outputs_tf_fold\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n        gc.collect()\n        logger.info(f\"Transformer Fold {fold_num+1} processing completed in {time.time() - fold_start_time:.2f} seconds.\")\n\n    # Log overall Transformer CV performance\n    mean_transformer_cv_score = np.mean(fold_transformer_val_scores)\n    logger.info(f\"\\n===== Transformer CV Summary =====\")\n    logger.info(f\"Fold Validation F1 Scores (Transformer): {fold_transformer_val_scores}\")\n    logger.info(f\"Mean Validation F1-micro (Transformer): {mean_transformer_cv_score:.5f}\")\n    \n    # Calculate and log overall OOF F1 for the Transformer model (for reference)\n    oof_transformer_pred_labels = np.argmax(oof_preds_transformer_logits, axis=1)\n    overall_oof_f1_transformer = f1_score(train_df['label'].values, oof_transformer_pred_labels, average=\"micro\", zero_division=0)\n    logger.info(f\"Overall OOF F1-micro (Transformer-only): {overall_oof_f1_transformer:.5f}\")\n\n\n    # --- STAGE 2: SYMBOLIC MODEL (LightGBM) ---\n    # This stage trains a LightGBM model on engineered symbolic and TF-IDF features.\n    # It provides an alternative perspective to the Transformer model.\n    oof_probs_symbolic = np.zeros((len(train_df), CONFIG['num_labels'])) # For OOF predictions from symbolic model\n    test_probs_symbolic = np.zeros((len(test_df), CONFIG['num_labels'])) # For test predictions\n    \n    if CONFIG['use_symbolic_model']:\n        logger.info(\"\\n===== Starting Stage 2: Symbolic Model (LightGBM) Training =====\")\n        \n        # Extract symbolic features from the 'Question' (raw text) column\n        train_sym_features_df = extract_symbolic_features(train_df['Question'])\n        test_sym_features_df = extract_symbolic_features(test_df['Question'])\n        \n        # Combine symbolic features with TF-IDF features\n        X_train_sym_full, sym_tfidf_vectorizer = combine_symbolic_and_tfidf(\n            train_sym_features_df, train_df['Question'], fit_vectorizer=True\n        )\n        X_test_sym_combined, _ = combine_symbolic_and_tfidf(\n            test_sym_features_df, test_df['Question'], fit_vectorizer=False, vectorizer=sym_tfidf_vectorizer\n        )\n        \n        y_train_full_sym = train_df['label'].values\n\n        # Create a train/validation split from the full symbolic training data for LGBM's early stopping\n        # This is separate from the Transformer's CV folds.\n        X_train_lgbm, X_val_lgbm, y_train_lgbm, y_val_lgbm = train_test_split(\n            X_train_sym_full, y_train_full_sym,\n            test_size=CONFIG[\"lgbm_eval_split_size\"],\n            random_state=CONFIG[\"seed\"],\n            stratify=y_train_full_sym # Ensure stratified split for imbalanced classes\n        )\n        logger.info(f\"LightGBM training set size: {X_train_lgbm.shape}, Validation set size: {X_val_lgbm.shape}\")\n        \n        lgbm_model = lgb.LGBMClassifier(**CONFIG[\"lgbm_params\"])\n        \n        # Callbacks for LightGBM: early stopping\n        lgbm_callbacks = []\n        if CONFIG[\"lgbm_params\"].get('n_estimators', 0) > 100 and CONFIG[\"lgbm_eval_split_size\"] > 0:\n            lgbm_callbacks.append(lgb.early_stopping(100, verbose=False)) # Early stopping rounds, verbose=False to keep logs clean\n        \n        logger.info(f\"Training LightGBM with early stopping (if applicable)...\")\n        lgbm_model.fit(\n            X_train_lgbm, y_train_lgbm,\n            eval_set=[(X_val_lgbm, y_val_lgbm)], # Provide evaluation set for early stopping\n            eval_metric=CONFIG[\"lgbm_params\"].get('metric', 'multi_logloss'), # Ensure eval_metric is passed\n            callbacks=lgbm_callbacks\n        )\n        \n        # After training with early stopping (or full training if no early stopping),\n        # for OOF and test predictions, it's common to retrain on the *full* symbolic training data\n        # or use the model trained with early stopping.\n        # For simplicity here, we'll use the model trained (possibly with early stopping) to predict on the full set.\n        # A more robust approach might involve CV for LGBM OOFs or retraining on X_train_sym_full.\n        \n        logger.info(\"Predicting probabilities with LightGBM...\")\n        # Get probabilities on the entire original training set (for ensembling with Transformer OOF)\n        oof_probs_symbolic = lgbm_model.predict_proba(X_train_sym_full)\n        test_probs_symbolic = lgbm_model.predict_proba(X_test_sym_combined) # Probabilities on test data\n        \n        logger.info(\"Symbolic feature model (LightGBM) trained and predictions made.\")\n        \n        # Display feature importances from LightGBM (useful for understanding the model)\n        try:\n            feature_importances_df = pd.DataFrame({\n                'feature': X_train_lgbm.columns, # Use columns from training data used for fit\n                'importance': lgbm_model.feature_importances_\n            }).sort_values('importance', ascending=False)\n            logger.info(\"Top 20 Symbolic Model Feature Importances:\")\n            logger.info(f\"\\n{feature_importances_df.head(20)}\")\n        except Exception as e: logger.warning(f\"Could not get LGBM feature importances: {e}\")\n        \n        # Clean up large DataFrames\n        del X_train_sym_full, X_test_sym_combined, lgbm_model, train_sym_features_df, test_sym_features_df\n        del X_train_lgbm, X_val_lgbm, y_train_lgbm, y_val_lgbm\n        gc.collect()\n    else:\n        logger.info(\"Symbolic model is disabled by configuration. Using zero probabilities for its contribution.\")\n        # Ensure symbolic probabilities are correctly shaped zeros if the model is not used.\n        # This prevents errors if ensemble weights are non-zero by mistake.\n        oof_probs_symbolic = np.zeros_like(oof_preds_transformer_logits) # Match shape of transformer OOF logits\n        test_probs_symbolic = np.zeros((len(test_df), CONFIG['num_labels'])) # Match shape of test predictions\n\n\n    # --- ENSEMBLING & SUBMISSION ---\n    logger.info(\"\\n===== Performing Ensembling and Creating Submission =====\")\n    \n    # Convert Transformer logits to probabilities using softmax\n    oof_probs_transformer = softmax(oof_preds_transformer_logits, axis=1)\n    \n    # Average test Transformer logits across folds, then convert to probabilities\n    mean_test_transformer_logits = np.mean(test_preds_transformer_logits_folds, axis=1)\n    test_probs_transformer = softmax(mean_test_transformer_logits, axis=1)\n\n    # Weighted Averaging of probabilities from Transformer and Symbolic model\n    w_t = CONFIG['ensemble_transformer_weight']\n    w_s = CONFIG['ensemble_symbolic_weight']\n    \n    # If symbolic model was not used, adjust weights to give full importance to Transformer.\n    if not CONFIG['use_symbolic_model']:\n        logger.info(\"Symbolic model not used, adjusting ensemble weights for Transformer-only.\")\n        w_t = 1.0\n        w_s = 0.0\n        # Re-ensure symbolic probs are correctly shaped zeros\n        if not np.array_equal(oof_probs_symbolic.shape, oof_probs_transformer.shape):\n             oof_probs_symbolic = np.zeros_like(oof_probs_transformer)\n        if not np.array_equal(test_probs_symbolic.shape, test_probs_transformer.shape):\n             test_probs_symbolic = np.zeros_like(test_probs_transformer)\n\n    logger.info(f\"Combining probabilities with weights: Transformer={w_t}, Symbolic={w_s}\")\n    combined_oof_probs = (w_t * oof_probs_transformer) + (w_s * oof_probs_symbolic)\n    combined_test_probs = (w_t * test_probs_transformer) + (w_s * test_probs_symbolic)\n\n    # Get final predicted labels by taking argmax of combined probabilities\n    final_oof_pred_labels = np.argmax(combined_oof_probs, axis=1)\n    final_test_pred_labels = np.argmax(combined_test_probs, axis=1)\n\n    # --- Evaluate Combined OOF Predictions ---\n    overall_oof_combined_f1 = f1_score(train_df['label'].values, final_oof_pred_labels, average=\"micro\", zero_division=0)\n    logger.info(f\"Overall COMBINED OOF F1-micro: {overall_oof_combined_f1:.5f}\")\n    improvement_over_transformer = overall_oof_combined_f1 - overall_oof_f1_transformer\n    logger.info(f\"(Improvement over Transformer-only OOF: {improvement_over_transformer:+.5f})\")\n\n    # --- Create Submission File ---\n    logger.info(\"Creating submission file...\")\n    submission_df = pd.DataFrame({'id': test_df['id'], 'label': final_test_pred_labels})\n    \n    # Construct a descriptive filename for the submission\n    submission_filename_parts = [CONFIG[\"submission_filename_prefix\"]]\n    submission_filename_parts.append(CONFIG['base_model_name'].split('/')[-1])\n    submission_filename_parts.append(f\"Preproc{CONFIG['transformer_preprocess_strategy']}\")\n    if CONFIG['use_contrastive_pretraining'] and SENTENCE_TRANSFORMERS_AVAILABLE:\n        submission_filename_parts.append(\"Contrastive\")\n    if CONFIG['use_symbolic_model']:\n        submission_filename_parts.append(\"SymLGBM\")\n    submission_filename_parts.append(f\"folds{CONFIG['transformer_n_splits']}\")\n    submission_filename_parts.append(f\"oof{overall_oof_combined_f1:.4f}.csv\")\n    detailed_submission_filename = \"_\".join(submission_filename_parts)\n    \n    # Use \"submission.csv\" if it exists (standard Kaggle name), otherwise use detailed name.\n    # This helps with direct submission on Kaggle.\n    submission_filepath_kaggle = \"submission.csv\" # Standard Kaggle output name\n    submission_filepath_detailed = os.path.join(CONFIG['output_dir_base'], detailed_submission_filename)\n\n    # Determine final path: if /kaggle/working/submission.csv would be created, use that for auto-detection\n    # otherwise, save to our designated output directory with the detailed name.\n    # This logic assumes we are in a Kaggle environment if we can write to \"submission.csv\" at root.\n    # A more robust check might involve checking environment variables.\n    final_submission_path_to_use = submission_filepath_kaggle\n    # If not in a typical Kaggle writable root, or if \"submission.csv\" isn't the expected output,\n    # default to the detailed path. For simplicity, we'll try to write to \"submission.csv\"\n    # and if that fails (e.g. permissions or not Kaggle), we'll use the detailed path.\n    # A common Kaggle pattern is just to name it \"submission.csv\".\n    \n    try:\n        submission_df.to_csv(final_submission_path_to_use, index=False)\n        logger.info(f\"Submission file created: {final_submission_path_to_use}\")\n        # Also save with the detailed name if it's different, for record-keeping\n        if final_submission_path_to_use != submission_filepath_detailed:\n            # Ensure the directory for the detailed path exists\n            os.makedirs(os.path.dirname(submission_filepath_detailed), exist_ok=True)\n            shutil.copyfile(final_submission_path_to_use, submission_filepath_detailed)\n            logger.info(f\"Submission also saved as: {submission_filepath_detailed}\")\n    except Exception as e:\n        logger.error(f\"Failed to save submission file '{final_submission_path_to_use}': {e}\")\n        # Fallback to saving with detailed name if primary path fails\n        try:\n            os.makedirs(os.path.dirname(submission_filepath_detailed), exist_ok=True)\n            submission_df.to_csv(submission_filepath_detailed, index=False)\n            logger.info(f\"Submission file saved with detailed name: {submission_filepath_detailed}\")\n        except Exception as e2:\n            logger.error(f\"Also failed to save submission file with detailed name '{submission_filepath_detailed}': {e2}\")\n\n\n    # --- Save OOF Predictions (Good Practice for Analysis) ---\n    logger.info(\"Saving OOF predictions for analysis...\")\n    oof_save_df = pd.DataFrame({\n        'id': train_df.get('id', train_df.index), # Use 'id' if available, else index\n        'true_label': train_df['label']\n    })\n    # Add logits/probabilities from each model and combined predictions\n    for i in range(CONFIG['num_labels']):\n        oof_save_df[f'transformer_logit_{i}'] = oof_preds_transformer_logits[:, i]\n        if CONFIG['use_symbolic_model']:\n             oof_save_df[f'symbolic_prob_{i}'] = oof_probs_symbolic[:, i]\n        oof_save_df[f'combined_prob_{i}'] = combined_oof_probs[:, i]\n    oof_save_df['transformer_pred_label'] = oof_transformer_pred_labels\n    oof_save_df['final_oof_pred_label'] = final_oof_pred_labels\n    \n    oof_filename = detailed_submission_filename.replace(\".csv\", \"_OOF.csv\").replace(\"submission_\", \"oof_\")\n    oof_filepath = os.path.join(CONFIG['output_dir_base'], oof_filename)\n\n    try:\n        os.makedirs(os.path.dirname(oof_filepath), exist_ok=True)\n        oof_save_df.to_csv(oof_filepath, index=False)\n        logger.info(f\"OOF predictions saved: {oof_filepath}\")\n    except Exception as e:\n        logger.error(f\"Failed to save OOF predictions file: {e}\")\n\n    logger.info(\"--- Math Problem Classification Pipeline Finished Successfully ---\")\n\n\n# ==============================================================================\n# Entry Point for Script Execution\n# ==============================================================================\nif __name__ == \"__main__\":\n    # The main function encapsulates the entire pipeline.\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-19T08:23:13.830434Z","iopub.execute_input":"2025-05-19T08:23:13.831141Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"hello\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-19T08:23:05.192Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}